\chapter{Model order reduction for nonlinear dynamical systems}
\label{MOR_chap}
In this chapter we present two methods for model order reduction of nonlinear dynamical systems. First, we present the method of proper orthogonal decomposition (POD) which derives a low-dimensional subspace $U_\ell$ such that a Galerkin projection onto that subspace still captures most of the dynamical behavior of the original system. An improvement of POD that also reduces the dimension of the involved nonlinearity is given by the discrete empirical interpolation method (DEIM). We end that chapter with an application of POD-DEIM to the nonlinear Burgers' equation and present both approximation results and results of the gain in computational complexity.
\section{Proper orthogonal decomposition (POD)}
The focus of this chapter lies on nonlinear dynamical systems of the form
\begin{align}
 \label{NonLinSys}
 \frac{d}{dt} \mathbf{y}(t) &= A \mathbf{y}(t) + \mathbf{F}(t,\mathbf{y}(t)), \\
 \label{NonLinSysIC}
 \mathbf{y}(0) &= \mathbf{y}_0,
\end{align}
which for example arise after spatial discretization of time-dependent nonlinear partial differential equations (PDEs). Therefore, we assume that the unknowns vector $\mathbf{y}$ is of dimension $n$ and the function $\mathbf{F}:[0,T] \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ captures the nonlinearity of the dynamical behavior. We have chosen the form \eqref{NonLinSys} in order to stress the difference between a linear and a nonlinear term in the dynamical behavior. We will see that a POD-reduction leads directly to a dimension reduction of the linear term while the evaluation of the nonlinear term still depends on the full dimension $n$.
\subsection{Optimality of the POD-basis}
The overall aim of POD is the projection of the governing equations \eqref{NonLinSys}-\eqref{NonLinSysIC} onto a suitable subspace $\mathcal{U}$ of dimension $\ell \ll n$ which captures most of the dynamical behavior of the original system. In order to obtain this subspace, the so-called matrix of snapshots defined as
\begin{align}
\label{snapshots}
Y := [\mathbf{y}(t_1), \mathbf{y}(t_2), ..., \mathbf{y}(t_{n_s})] \in \mathbb{R}^{n \times n_s},
\end{align}
plays a key role. The solution vector $\mathbf{y}$ at $n_s$ different time instances form the columns of the matrix $Y$. The integer $n_s$ is called the number of snapshots and typically, $n_s \ll n$.

In this section, we will explain that a singular value decomposition (SVD) of the snapshot matrix \eqref{snapshots} can be used to obtain an optimal projection space. In more detail, we present a proof from \cite{V11} that shows that the optimal $\ell$-dimensional projection space is given by those $\ell$ singular vectors of $Y$ that correspond to the $\ell$ largest singular values. The singular value decomposition of a rectangular matrix $Y$ is given by the following well-known theorem:
\begin{theorem}
\emph{(Singular value decomposition, \cite{G96})}
\label{svdthm}
Let $Y \in \mathbb{R}^{n \times n_s}$ of rank $d$. Then there exists a decomposition of the form
\begin{align}
\label{svd}
Y = U \bbmat D & 0 \\ 0 & 0 \ebmat V^T =: U \Sigma V^T,
\end{align}
with $U \in \mathbb{R}^{n \times n}, V \in \mathbb{R}^{n_s \times n_s}$ orthogonal and $D = diag(\sigma_1,...,\sigma_d) \in \mathbb{R}_+^{d \times d}$. The columns in $U = [\mathbf{u}_1,...,\mathbf{u}_n]$ are called the (left) singular vectors of $Y$ and for the singular values $\sigma_i$ it holds: $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_d > 0$.
\end{theorem}
\begin{proof}
Can, for example, be found in \cite[Theorem 2.5.2]{G96}.
\end{proof}
Note that the following diagonalization holds
\begin{align}
\label{SVDalsEW}
Y Y^T = (U \Sigma V^T) (U \Sigma V^T)^T = U \Sigma^2 U^T,
\end{align}
and, hence, the columns of $U$ are eigenvectors of $Y Y^T$ with eigenvalues $\lambda_i = \sigma_i^2 > 0, \ i=1,...,d$.

We will next consider the optimization problem \eqref{optBasis} whose solution gives rise to the practical computation of the POD-basis. We seek for an orthonormal basis $\varphi_1,...,\varphi_\ell$ such that the components of the snapshots $\mathbf{y}(t_1),...,\mathbf{y}(t_{n_s})$ are maximized when expressed in this basis. Therefore, it is desired to find the basis $\varphi_1,...,\varphi_\ell$ and the following theorem states that the left singular vectors of $Y$ solve the optimization problem \eqref{optBasis}.
%The next theorem shows that the solution to the optimization problem \eqref{optBasis} is given by the left singular vectors of the snapshot matrix \eqref{snapshots}. This is important since in \eqref{optBasis} we seek for an orthonormal basis $\varphi_1,...,\varphi_\ell$ such that the components of the snapshots $\mathbf{y}(t_1),...,\mathbf{y}(t_{n_s})$ are maximized when expressed in this basis. Therefore, it is desired to find the basis $\varphi_1,...,\varphi_\ell$ and the following theorem states that the left singular vectors of $Y$ solve \eqref{optBasis}.
\begin{theorem}
\emph{(POD basis, \cite{V11})}
\label{podthm}
Let $Y \in \mathbb{R}^{n \times n_s}$ be the snapshot matrix \eqref{snapshots} with rank $d \leq \min \{n,n_s\}$. Further, let $Y = U \Sigma V^T$ be the singular value decomposition of $Y$ with orthogonal matrices $U = [\mathbf{u}_1,...,\mathbf{u}_n]$ and $V = [\mathbf{v}_1,...,\mathbf{v}_{n_s}]$ as in \eqref{svd}. Then, for any $\ell \in \{1,...,d\}$ the solution to the optimization problem
\begin{align}
\label{optBasis}
\max_{\varphi_1,...,\varphi_\ell} \sum_{i=1}^\ell \sum_{j=1}^{n_s} |\langle \mathbf{y}_j , \varphi_i \rangle|^2 \quad s.t. \quad \langle \varphi_i,\varphi_j\rangle = \delta_{i,j} \ \text{for } 1 \leq i,j \leq \ell
\end{align}
is given by the left singular vectors $\{ \mathbf{u}_i \}_{i=1}^\ell$. The vectors $\varphi_1,...,\varphi_\ell$ are called the POD basis of rank $\ell$. Hereby, $\delta_{i,j}$ denotes the Kronecker delta.
\end{theorem}
\begin{proof}
The following proof is based on the ideas of \cite{V11}. Since \eqref{optBasis} is an equality constrained optimization problem, we consider the corresponding Lagrangian function
\begin{align*}
&\mathcal{L} : \mathbb{R}^n \times ... \times \mathbb{R}^n \times \mathbb{R}^{\ell \times \ell} \rightarrow \mathbb{R}\\
&\mathcal{L}(\varphi_1,...,\varphi_\ell,\Lambda) = \sum_{i=1}^\ell \sum_{j=1}^{n_s} |\langle \mathbf{y}_j , \varphi_i \rangle|^2 + \sum_{i,j=1}^\ell \lambda_{ij} (\delta_{ij}-\langle \varphi_i,\varphi_j\rangle),
\end{align*}
where $\Lambda := (\lambda_{i,j})_{i,j=1}^\ell$. The first-order necessary optimality conditions of \eqref{optBasis} are then given by
\begin{align*}
\frac{\partial \mathcal{L}}{\partial \varphi_k}(\varphi_1,...,\varphi_\ell,\Lambda) &= 0, \quad \text{for } k \in \{1,...,\ell\}, \\
\frac{\partial \mathcal{L}}{\partial \lambda_{i,j}}(\varphi_1,...,\varphi_\ell,\Lambda) &= 0, \quad \text{for } i,j \in \{1,...,\ell\},
\end{align*}
where the latter equation is equivalent to the constraint $\langle \varphi_i,\varphi_j\rangle = \delta_{i,j}$. In \cite[p. 5]{V11}, it is shown that the first condition leads to
\begin{align}
\label{zwschritt}
Y Y^T \varphi_k = \frac{1}{2} \sum_{i=1}^\ell (\lambda_{i,k} + \lambda_{k,i}) \varphi_i, \quad \text{for all } k \in \{1,...,\ell\}.
\end{align}
The actual proof that the optimal vectors $\varphi_1,...,\varphi_\ell$ are given by the first $\ell$ left singular vectors of $Y$ is done by induction on $\ell$. First, let $\ell=1$. From \eqref{zwschritt}, it follows that $k=1$ and we get:
\begin{align*}
Y Y^T \varphi_1 = \lambda_{1,1} \varphi_1.
\end{align*}
From \eqref{SVDalsEW}, we have that this eigenequation can be expressed as an SVD with $\varphi_1$ being the first left singular vector of $Y$.

Next, consider the case $\ell \geq 1$ and suppose
\begin{align*}
Y Y^T \varphi_k = \lambda_{k,k} \varphi_k, \quad \text{for all } k \in \{1,...,\ell\}.
\end{align*}
We want to show that the first-order necessary optimality conditions for a POD-basis of rank $\ell+1$ are given by
\begin{align*}
Y Y^T \varphi_k = \lambda_{k,k} \varphi_k, \quad \text{for all } k \in \{1,...,\ell+1\},
\end{align*}
and can, thus, be computed by an SVD of $Y$. Using the induction hypothesis, we only have to show that
\begin{align*}
Y Y^T \varphi_{\ell+1} = \lambda_{\ell+1,\ell+1} \varphi_{\ell+1}.
\end{align*}
Since $\{\varphi_i\}_{i=1}^{\ell+1}$ is a POD-basis, it holds that $\langle \varphi_{\ell+1},\varphi_i \rangle = 0$ for $1 \leq i \leq \ell$ and we get
\begin{align*}
0 &= \lambda_{i,i} \langle \varphi_{\ell+1},\varphi_i \rangle = \langle \varphi_{\ell+1}, Y Y^T \varphi_i \rangle = \langle Y Y^T  \varphi_{\ell+1}, \varphi_i \rangle \\
 &= \frac{1}{2} \sum_{j=1}^{\ell+1} (\lambda_{j,\ell+1} + \lambda_{\ell+1,j}) \langle \varphi_j,\varphi_i \rangle = (\lambda_{i,\ell+1} + \lambda_{\ell+1,i}) \quad \Rightarrow \quad \lambda_{\ell+1,i} = - \lambda_{i,\ell+1}, \ i \in \{1,...,\ell\},
\end{align*}
where the symmetry of $Y Y^T$ guarantees the matrix to be self-adjoint in $\langle \cdot, \cdot \rangle_{\mathbb{R}^n}$.

Inserting the last relation into \eqref{zwschritt}, we obtain
\begin{align*}
Y Y^T \varphi_{\ell+1} &= \frac{1}{2} \sum_{i=1}^{\ell+1} (\lambda_{i,\ell+1}+\lambda_{\ell+1,i}) \varphi_{i} = \frac{1}{2} \sum_{i=1}^{\ell} (\lambda_{i,\ell+1}+\lambda_{\ell+1,i}) \varphi_{i} + \lambda_{\ell+1,\ell+1} \varphi_{\ell+1} \\
 &= \frac{1}{2} \sum_{i=1}^{\ell} \underbrace{(\lambda_{i,\ell+1}-\lambda_{i,\ell+1})}_{=0} \varphi_{i} + \lambda_{\ell+1,\ell+1} \varphi_{\ell+1} = \lambda_{\ell+1,\ell+1} \varphi_{\ell+1}
\end{align*}

We have shown that the first-order optimality conditions corresponding to \eqref{optBasis} are given by the $n \times n$ eigenvalue problems
\begin{align*}
Y Y^T \mathbf{u}_i = \lambda_i \mathbf{u}_i, \quad i = 1,...,\ell,
\end{align*}
where the vectors $\{\mathbf{u}_i\}_{i=1}^\ell$ are given by the left singular vectors of $Y$ due to \eqref{SVDalsEW}. The proof that $\{\mathbf{u}_i\}_{i=1}^\ell$ are a solution of \eqref{optBasis} can be found in \cite{V11}.
\end{proof}
For the practical usage of POD, we need to choose the dimension $\ell$ of the POD-basis. According to \cite{V11}, there exists no theoretical bound for the approximation error depending on $\ell$. Therefore, in practice the choice of $\ell$ has to be obtained heuristically. The ratio,
\begin{align}
\label{heuristics}
\varepsilon(\ell) = \frac{\sum_{i=1}^\ell \sigma_i^2}{\sum_{i=1}^n \sigma_i^2}, \quad 1 \leq \ell \leq n,
\end{align}
 gives a good estimate for the relation of the energy of the reduced system and the total energy. Note that $\varepsilon(\cdot)$ is growing fast when the considered dynamical system is suitable for model order reduction. For instance, in the setting of Section \ref{BurgersPODDEIM} we present the distribution of the singular values for $\nu = 0.01$ in Figure \ref{singVal}. The numerical solution of the full-order model required a spatial discretization of $n = 80$ and a POD-dimension of $\ell = 9$ already leads to $\varepsilon(9) = 0.98$ which gives a motivation that most of the dynamical behavior of the full-order system can be captured by a POD reduced model of dimension $9$ which can also be seen in Figure \ref{PODplot}.
\subsection{The projected reduced-order model}
In the previous section we have shown that the optimal subspace $\mathcal{U}$ that captures most of the dynamical behavior of the original system is given by $\mathcal{U} = span \{\mathbf{u}_1,...,\mathbf{u}_\ell\}$, where $\{\mathbf{u}_i\}_{i=1}^\ell$ are the POD-basis vectors of dimension $\ell$. In order to construct the reduced-order model, we define the matrix $U_\ell := [\mathbf{u}_1,...,\mathbf{u}_\ell]$. Then, the following ansatz gives an $\ell$-dimensional approximation of the solution vector $\mathbf{y}$:
\begin{align*}
\mathbf{y}(t) \approx U_\ell \mathbf{\tilde y}(t), \quad \mathbf{\tilde y}(t) \in \mathbb{R}^\ell.
\end{align*}
A reduced-order model of \eqref{NonLinSys}-\eqref{NonLinSysIC} for $\mathbf{\tilde y}(t)$ can be derived by a Galerkin projection of \eqref{NonLinSys} onto $U_\ell$,
\begin{align}
\label{GalProj}
\frac{d}{dt} \mathbf{\tilde y}(t) &= U_\ell^T A U_\ell \mathbf{\tilde y}(t) + U_\ell^T \mathbf{F}(t,U_\ell\mathbf{\tilde y}(t)) \nonumber \\
 &=: \tilde A \mathbf{\tilde y}(t) + \mathbf{\tilde N}(\mathbf{\tilde y}),
\end{align}
with $\tilde A := U_\ell^T A U_\ell \in \mathbb{R}^{\ell \times \ell}$ and non-linearity $\mathbf{\tilde N}(\mathbf{\tilde y}) : = U_\ell^T \mathbf{F}(t,U_\ell\mathbf{\tilde y}(t))$.

The corresponding initial condition is given by
\begin{align*}
\mathbf{\tilde y}(0) = U_\ell^T \mathbf{y}_0.
\end{align*}
\section{Discrete empirical interpolation method (DEIM)}
\label{Deim_chap}
In the previous sections, the POD-Galerkin approach \eqref{GalProj} has been derived to be a reduced-order model of the full order dynamical system \eqref{NonLinSys}-\eqref{NonLinSysIC}. The system \eqref{GalProj} is of the (small) dimension $\ell$ and the unknown $\mathbf{\tilde y}$ is an $\ell$-dimensional approximation of $\mathbf{y}$. However, the evaluation of the nonlinear term $\mathbf{\tilde N}(\mathbf{\tilde y})$ still has computational complexity that depends on the original problem size $n$ as the following consideration shows:
\begin{align*}
\mathbf{\tilde N}(\mathbf{\tilde y}) = \underbrace{U_\ell^T}_{\ell \times n} \underbrace{\mathbf{F}(U_\ell\mathbf{\tilde y}(t))}_{n \times 1}.
\end{align*}
The DEIM method is an efficient way to overcome this dependence on $n$ and is, therefore, an improvement of the POD algorithm, cf. \cite{DEIM}. For a more simple notation, let us denote from now on $\mathbf{f}(t) := \mathbf{F}(U_\ell\mathbf{\tilde y}(t))$. We are looking for a low-dimensional approximation
\begin{align*}
\mathbf{f}(t) \approx W \mathbf{c}(t),
\end{align*}
where $W := [\mathbf{w}_1,...,\mathbf{w}_m] \in \mathbb{R}^{n \times m}$ and coefficient vector $\mathbf{c}(t) \in \mathbb{R}^m$, similar to the POD-approach. Indeed, the projection space $span \{\mathbf{w}_1,...,\mathbf{w}_m\}$ is obtained by an SVD of the snapshot matrix of the nonlinearity
\begin{align}
\label{snapshotF}
F := [\mathbf{f}(t_1), \mathbf{f}(t_2), ..., \mathbf{f}(t_{n_s})] \in \mathbb{R}^{n \times n_s}.
\end{align}
Since $\mathbf{f}(t) = W \mathbf{c}(t)$ is an overdetermined linear system of equations for $\mathbf{c}(t)$, we select $m$ distinguished rows from both sides of the system. Therefore, we define the matrix $\mathcal{P} = [\mathbf{e}_{\wp_1},...,\mathbf{e}_{\wp_m}] \in \mathbb{R}^{n \times m}$ with $\mathbf{e}_{\wp_i} = [0,...,0,1,0,...,0] \in \mathbb{R}^n$ having its non-zero entry at the $i$-th component. If $\mathcal{P}^T W$ is nonsingular, the coefficient vector $\mathbf{c}(t)$ can uniquely be determined from
\begin{align*}
\mathcal{P}^T \mathbf{f}(t) = (\mathcal{P}^T W) \mathbf{c}(t).
\end{align*}
Altogether, we derive an approximation of $\mathbf{f}(t)$ of the following form
\begin{align}
\label{approxF}
\mathbf{f}(t) \approx W \mathbf{c}(t) = W (\mathcal{P}^T W)^{-1} \mathcal{P}^T \mathbf{f}(t) =: \mathbf{\hat f}(t),
\end{align}
where it is important to note that the matrix-vector multiplication $\mathcal{P}^T \mathbf{f}(t)$ is never computed in the standard way since this would imply a dependence of the computational complexity on $n$. Instead, a left multiplication with $\mathcal{P}^T$ is by construction equivalent to the selection of $m$ entries of the vector $\mathbf{f}(t)$ which has complexity $\mathcal{O}(m)$.

The nonlinear term in \eqref{GalProj} can, thus, be computed via
\begin{align*}
\mathbf{\tilde N}(\mathbf{\tilde y}) &\approx U_\ell^T W (\mathcal{P}^T W)^{-1} \mathcal{P}^T \mathbf{F}(U_\ell \mathbf{\tilde y}(t))\\
&\stackrel{(\ast)}{=}\underbrace{U_\ell^T W (\mathcal{P}^T W)^{-1}}_{\ell \times m} \underbrace{\mathbf{F}(\mathcal{P}^T U_\ell \mathbf{\tilde y}(t))}_{m \times 1},
\end{align*}
where we have assumed that the function $\mathbf{F}(\cdot)$ only acts pointwise on its input vector. Hence, it is possible in step $(\ast)$ to first select the $m$ components of the input vector and then evaluate the function $\mathbf{F}$. The resulting approximation of $\mathbf{\tilde N}(\mathbf{\tilde y})$ does not depend on the dimension $n$ of the full order system and, moreover, we note that the matrix $U_\ell^T W (\mathcal{P}^T W)^{-1}$ does not depend on time and can, thus, be pre-computed.

It remains to present how the $m$ entries of $\mathbf{f}(t)$ have to be selected such that the approximation $\mathbf{\hat f}(t)$ in formula \eqref{approxF} is optimal. In \cite[Section 3.1]{DEIM}, algorithm \ref{alg:DEIM} is presented which for a given input basis $\mathbf{w}_1,...,\mathbf{w}_m$ successively builds the matrix $\mathcal{P}$. In the initialization step, the first index $\wp_1 \in \{1,...,n\}$ is selected corresponding to the largest component in magnitude of the first input basis vector $\mathbf{w}_1$. The loop over $i=2$ to $i=m$ selects the indices corresponding to the largest component of the residual $\mathbf{r}$ between the input basis $\mathbf{w}_i$ and its approximation within the subspace $span \{\mathbf{w}_1,...,\mathbf{w}_{i-1}\}$.
\begin{algorithm}[H]
\caption{The DEIM algorithm, \cite{DEIM}}
\label{alg:DEIM}
\begin{algorithmic}[1]
\STATE \textbf{INPUT: } $\{\mathbf{w}_i\}_{i=1}^m \subset \mathbb{R}^n$  linear independent
\STATE \textbf{OUTPUT: } $\vec \wp = [\wp_1,...,\wp_m]^T \in \mathbb{R}^m, \ \mathcal{P} \in \mathbb{R}^{n \times m}$
\STATE $[|\rho|,\wp_1] = \max \{|\mathbf{w}_1|\}$
\STATE $W = [\mathbf{w}_1], \mathcal{P} = [\mathbf{e}_{\wp_1}], \vec \wp = [\wp_1]$
\FOR{$i = 2$ to $m$}
\STATE Solve $(\mathcal{P}^T W) \mathbf{c} = \mathcal{P}^T \mathbf{w}_i$ for $\mathbf{c}$
\STATE $\mathbf{r} = \mathbf{w}_i - W \mathbf{c}$
\STATE $[|\rho|,\wp_i] = \max \{|\mathbf{r}|\}$
\STATE $W \leftarrow [W \ \mathbf{w}_i], \mathcal{P} \leftarrow [\mathcal{P} \ \mathbf{e}_{\wp_i}], \vec \wp \leftarrow \begin{bmatrix} \vec \wp \\ \wp_i \end{bmatrix}$
\ENDFOR
\end{algorithmic}
\end{algorithm}
An illustrative example taken from \cite{DEIM} is given by the nonlinear parameterized function
\begin{align}
\label{1Dex}
s(x,\mu) = (1-x) \cos(3 \pi \mu (x+1)) e^{-(1+x)\mu}, \quad x \in [-1,1], \mu \in [1,\pi],
\end{align}
where the DEIM algorithm has been applied based on $100$ equidistantly spaced points $x_i \in [-1,1]$ and $51$ snapshots for $\mu_j \in [1,\pi]$. Figure \ref{DeimPoints} shows on the left that the first DEIM indices obtained by Algorithm \ref{alg:DEIM} are chosen in a region where most of the dynamics of $s$ occur. In the right plot of Figure \ref{DeimPoints}, it can be seen that for $\mu = 3.1$ the approximation $\hat s$ based on DEIM dimension $m=10$ gives a good result compared to the $100$ dimensional original model.
\begin{figure}[H]
\centering
\subfloat[First six POD bases with DEIM points.]{\includegraphics[width=0.49\textwidth]{plots/DeimPoints}}\hfill
\subfloat[DEIM approximation of dimension $10$.]{\includegraphics[width=0.49\textwidth]{plots/DeimApprox1D}}
\caption{The location of the DEIM indices of example \eqref{1Dex}.}\label{DeimPoints}
\end{figure}
%\subsubsection{Error bound for DEIM}
\begin{theorem}
\label{DEIMerrBound}
\emph{(Error bound of DEIM approximation, \cite{DEIM})}
\label{deimthm}
Let $\mathbf{f} \in \mathbb{R}^n$ be an arbitrary vector. Let $\{\mathbf{w}_i\}_{i=1}^m$ be the first $m$ left singular vectors of the snapshot matrix \eqref{snapshotF} (POD-basis). From \eqref{approxF}, the DEIM approximation of order $m$ for $\mathbf{f}$ in the space $span \{\mathbf{u}_1,...,\mathbf{u}_m\}$ is
\begin{align*}
\mathbf{\hat f} = W(\mathcal{P}^T W)^{-1}\mathcal{P}^T \mathbf{f},
\end{align*}
where $W := [\mathbf{w}_1,...,\mathbf{w}_m] \in \mathbb{R}^{n \times m}$ and $\mathcal{P} := [\mathbf{e}_{\wp_1},...,\mathbf{e}_{\wp_m}] \in \mathbb{R}^{n \times m}$, with $\{\wp_1,...,\wp_m\}$ being obtained from Algorithm \ref{alg:DEIM} with input basis $\{\mathbf{w}_1,...,\mathbf{w}_m\}$. An error bound for $\mathbf{\hat f}$ is then given by
\begin{align*}
\|\mathbf{f} - \mathbf{\hat f}\|_2 \leq \mathcal{C} \cdot \mathcal{E}_*(\mathbf{f}),
\end{align*}
where
\begin{align*}
\mathcal{C} = \|(\mathcal{P}^T W)^{-1}\|_2, \quad \mathcal{E}_*(\mathbf{f}) = \|(I-W W^T)\mathbf{f}\|_2.
\end{align*}
The constant $\mathcal{C}$ is bounded by
\begin{align*}
\mathcal{C} \leq \frac{(1+\sqrt{2n})^{m-1}}{|\mathbf{e}^T_{\wp_1}\mathbf{w}_1|} = (1+\sqrt{2n})^{m-1} \|\mathbf{w}_1\|_\infty^{-1}.
\end{align*}
\end{theorem}
\begin{proof}
The proof is given in \cite[p. 2747-2749]{DEIM}.
\end{proof}
Theorem \ref{DEIMerrBound} not only gives an error bound on the approximation obtained by the DEIM algorithm, but the proof also points out that the choice of the DEIM indices in Algorithm \ref{alg:DEIM} in fact minimizes the growth of $\|(\mathcal{P}^T W)^{-1}\|_2$. Therefore, the algorithm is optimal in the sense that the approximation error is minimized.
\section{Application: POD-DEIM for the unsteady Burgers' equation}
\label{BurgersPODDEIM}
The following example is based on the considerations in \cite{KV99} where the one-dimensional Burgers' equations together with homogeneous Dirichlet boundary conditions and a step function $y_0(x)$ as initial condition has been considered,
\begin{equation}
\label{Burgers1}
\begin{split}
y_t + \left( \frac{1}{2}y^2 - \nu y_x\right)_x = f &, \quad (x,t) \in (0,L) \times (0,T), \\
y(t,0) = y(t,L) = 0&, \quad t \in (0,T), \\
y(0,x) = y_0(x)&, \quad x \in (0,L).
\end{split}
\end{equation}
Burgers' equation is a fundamental partial differential equation (PDE) from fluid dynamics and is, for instance, used in gas dynamics. The formulation \eqref{Burgers1} is known to be the conservative form of Burgers' equation. Note that the numerical solution highly depends on the viscosity parameter $\nu$: For small $\nu$, the nonlinear term influences the numerical solution more and the PDE is called \textit{stiff}. This requires a small time step for the time integration.

For the numerical solution of the full-order system \eqref{Burgers1}, we used a finite element discretization in space using linear basis functions as described in  Appendix \ref{FEMDiscr_space}. This approach leads to the following system of ODEs
\begin{align}
\label{FEMdiscr}
M \mathbf{\dot y}(t) = -\frac{1}{2} B \mathbf{y}^2(t) - \nu C \mathbf{y}(t) + \mathbf{f}
\end{align}
where $M$ is the mass matrix, $C$ is the stiffness matrix, $B$ represents the convective term and, for simplicity, we assume in the following considerations the source term to be equal to zero, i.e. $f \equiv 0$. At this point, we also need to specify the initial condition $y_0(x)$,
\begin{align}
 \label{FEMdiscr_init}
 y_0(x) = \begin{cases} 1, & \text{ if } 0 \leq x \leq \frac{L}{2} \\ 0, & \text{ if } \frac{L}{2} < x \leq L \end{cases} \quad \Rightarrow \quad \mathbf{y}(0) = [1,...,1,0,...,0]^T \in \mathbb{R}^n.
\end{align}

The system \eqref{FEMdiscr} with initial condition \eqref{FEMdiscr_init} can be integrated in time using the implicit Euler method, see Appendix \ref{implEuler}.
\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.33\textwidth]{plots/FullModel_nu01}}\hfill
\subfloat{\includegraphics[width=0.33\textwidth]{plots/FullModel_nu001}}\hfill
\subfloat{\includegraphics[width=0.33\textwidth]{plots/FullModel_nu0001}}\hfill
\caption{Numerical solution of the full-order Burgers' equation for different viscosity parameters $\nu$ and initial condition \eqref{FEMdiscr_init}.}\label{FullNumSol}
\end{figure}
According to \cite{KV99} the POD-basis $\{\varphi_i\}_{i=1}^\ell$ can be obtained by solving the eigenvalue problem
\begin{align}
\label{PODviaEW}
Y Y^T M \varphi = \sigma^2 \varphi,
\end{align}
where $Y$ is the matrix of snapshots as previously defined in \eqref{snapshots}, i.e. $Y$ consists columnwise of the solution of Burgers' equation at different time instances.

Since the mass matrix $M$ is symmetric and positive definite, there exists a Cholesky decomposition of the form $M = R^T R$ and the eigenequation \eqref{PODviaEW} can instead be solved by an SVD of the matrix $RY$,
\begin{align}
\label{SVD_ss}
R Y = U \Sigma V^T.
\end{align}
After choosing a suitable POD-dimension $\ell \ll n$, let us define the matrix of left singular vectors, $U_{\ell} := [u_1,...,u_{\ell}]$, where $u_i$ are the columns of the matrix $U$ in \eqref{SVD_ss}. Then, the POD-basis is given by
\begin{align}
\label{Phidef}
\Phi_{\ell} := R^{-1} U_{\ell}
\end{align}
and the following ansatz leads to a reduced system,
\begin{align}
\label{PODansatz}
\mathbf{y}(t) \approx \Phi_{\ell} \mathbf{\tilde y}(t) \quad \text{with } \mathbf{\tilde y} \in \mathbb{R}^{\ell}.
\end{align}
The Galerkin projection of \eqref{FEMdiscr} onto $span \{\varphi_1,...,\varphi_\ell\}$ is given by
\begin{align}
\label{redSys}
\mathbf{\dot{\tilde y}}(t) = -\frac{1}{2} B_{\ell} (\Phi_{\ell} \mathbf{\tilde y})^2 - \nu C_{\ell} \mathbf{\tilde y},
\end{align}
with the matrices
\begin{align}
\label{Bl}
B_{\ell} &:= \Phi_{\ell}^T B \in \mathbb{R}^{\ell \times n},\\
\label{Cl}
C_{\ell} &:= \Phi_{\ell}^T C \Phi_{\ell} \in \mathbb{R}^{\ell \times \ell},
\end{align}
and the new mass matrix being equal to the $\ell \times \ell$ identity as the following calculation shows,
\begin{align}
\label{Mortho}
M_{\ell} &:= \Phi_{\ell}^T M \Phi_{\ell} = U_{\ell}^T U_{\ell} = I_{\ell} \in \mathbb{R}^{\ell \times \ell}.
\end{align}
We will refer to \eqref{redSys} as the POD-reduced system. As shown in \eqref{Mortho}, due to the M-orthogonality of the projection matrix $\Phi_\ell$, the mass matrix in the POD-reduced system vanishes which leads to a simpler numerical treatment of \eqref{redSys} compared to the full-order system \eqref{FEMdiscr}. Note, that $\Phi_\ell \mathbf{\tilde y} \in \mathbb{R}^n$ is still of large dimension and, therefore, no dimension reduction for the nonlinearity has been obtained so far. Also, the number of columns of the reduced matrix $B_\ell$ still depends on $n$.

The initial condition of the POD-reduced system transforms as follows,
\begin{align*}
\Phi_{\ell} \mathbf{\tilde y}(0) &= \mathbf{y}(0), \\
R^{-1} U_{\ell} \mathbf{\tilde y}(0) &= \mathbf{y}(0),
\end{align*}
and, therefore, the initial condition for \eqref{redSys} can be obtained by pre-multiplication of the full-size initial condition with the matrix product $\Phi_{\ell}^T M$ as shown below,
\begin{align}
\label{initRed}
\mathbf{\tilde y}(0) &= U_{\ell}^T R \mathbf{y}(0) = \Phi_{\ell}^T R^T R \mathbf{y}(0) = \Phi_{\ell}^T M \mathbf{y}(0).
\end{align}
Note that the initial condition of the full-order system is given in \eqref{FEMdiscr_init} and, thus, the respective initial condition of the reduced model follows directly from \eqref{initRed}.

As it has already been pointed out, the nonlinear term in \eqref{redSys} is still of large dimension $n$. Therefore, we apply the DEIM method in a next step in order to reduce the computational effort of evaluating the nonlinearity. Consider the snapshot matrix of the nonlinearity, \mbox{$F := [\mathbf{y}(t_1)^2,...,\mathbf{y}(t_{n_s})^2]$} and the corresponding singular value decomposition,
\begin{align*}
 F = U_f \Sigma_f V_f^T.
\end{align*}
Again, by choosing the size of the DEIM-dimension $m$, we are able to define the matrix of left singular vectors, $U_{m} := [u^f_1,...,u^f_{m}]^T$, which is used as an input basis for \mbox{algorithm \ref{alg:DEIM}}. From \mbox{algorithm \ref{alg:DEIM}}, we obtain the projection matrix $\mathcal{P}$ and the nonlinear term becomes:
\begin{align*}
 \Phi_{\ell}^T B (\Phi_{\ell} \mathbf{\tilde y})^2 = \Phi_{\ell}^T B U_{m} (\mathcal{P}^T U_{m})^{-1} \mathcal{P}^T (\Phi_{\ell} \mathbf{\tilde y})^2 \stackrel{(*)}{=} \underbrace{\Phi_{\ell}^T B U_{m} (\mathcal{P}^T U_{m})^{-1}}_{\ell \times m}(\underbrace{\mathcal{P}^T \Phi_{\ell}}_{m \times \ell} \mathbf{\tilde y})^2,
\end{align*}
where the step $(*)$ is allowed since squaring is a componentwise operation.
\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.49\textwidth]{plots/singValues_bignu}}\hfill
\subfloat{\includegraphics[width=0.49\textwidth]{plots/singValues_smallnu}}\hfill
\caption{Distribution of the singular values for $\nu = 0.01$ (left) and $\nu = 0.001$ (right).}\label{singVal}
\end{figure}
The fully reduced POD-DEIM system is then given by
\begin{align}
\label{finalPODDEIM}
\mathbf{\dot{\tilde y}} = -\frac{1}{2} \tilde B (\tilde F \mathbf{\tilde y})^2 - \nu \tilde C \mathbf{\tilde y} + \mathbf{\tilde f},
\end{align}
where
\begin{align}
\label{Bred}
\tilde{B} &:= \Phi_\ell^T B U_f(\mathcal{P}^T U_f)^{-1} \in \mathbb{R}^{\ell \times m},\\
\label{Cred}
\tilde{C} &:= \Phi_\ell^T C \Phi_\ell \in \mathbb{R}^{\ell \times \ell},\\
\label{Fred}
\tilde{F} &:= \mathcal{P}^T \Phi_\ell \in \mathbb{R}^{m \times \ell}.
\end{align}
Note that in a fast implementation, both matrices $\tilde{B}$ and $\tilde{F} $ can be pre-computed such that the system \eqref{finalPODDEIM} is indeed of dimension $\ell \times \ell$ and no dependence on the original \mbox{size $n$} exists anymore. The POD-DEIM reduced system \eqref{finalPODDEIM} as well as the full-order system \eqref{FEMdiscr} have been solved by the implicit Euler method, see Appendix \ref{implEuler}. The choice of the reduced dimension $\ell,m$ is crucial in order to obtain an accurate result of the reduced system. A good estimate for the choice of $\ell,m$ is given by the ration of the truncated sum of the singular values and the sum of all singular values given in \eqref{heuristics}. Therefore, we consider the distribution of the singular values of the respective snapshot matrices in Figure \ref{singVal}. We see that especially for $\nu = 0.01$, the first singular values decay tremendously which gives rise to a good approximation of the reduced system when only a small number for $\ell,m$ is chosen. We will present in Figure \ref{PODplot} the behavior of the numerical solution of the POD-DEIM reduced system when the DEIM-dimension $m$ is fixed and $\ell$ is increased.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{plots/PODplot_l.eps}\\
  \caption{POD-DEIM approximations for different dimensions $\ell$ and $m = 15$, $\nu = 0.01$.}\label{PODplot}
\end{figure}
\newpage
Figure \ref{PODplot} shows to convergence of the POD-DEIM reduced model to the full-order solution: In each computation a fixed DEIM dimension of $m = 15$ has been used to approximate the nonlinear term of Burgers' equation. If the dimension of the POD-basis $\ell$ and, thus, the size of the projected model is increased, we see from Figure \ref{PODplot} that the numerical solution of the reduced system converges towards a smooth solution that corresponds to the solution of the original model.
\subsection{Approximation error of the reduced model}
In order to show the convergence of the reduced POD-DEIM model to the full-order system in more detail, we computed both, the numerical solution of the full-order system $\mathbf{y}$ as well as the numerical solution of the reduced-order model which we denote by $\mathbf{y}^{(\ell,m)}$. By the superscript we mean that $\mathbf{y}^{(\ell,m)}(t) = \Phi_\ell \mathbf{\tilde y}(t)$, where the reduced variable $\mathbf{\tilde y}$ has been obtained by the POD-DEIM reduced model \eqref{finalPODDEIM} with POD-dimension $\ell$ and DEIM-dimension $m$, respectively. For $\Omega = [0,L] \times [0,T]$, with $L = T = 1$, we define the relative error $e_{\ell}$ using an approximation of the $L_2(\Omega)$ norm as:
\begin{align}
\label{relErr_def}
e_{\ell,m} := \frac{\|\mathbf{y} - \mathbf{y}^{(\ell,m)}\|_{L_2(\Omega)}}{\|\mathbf{y}\|_{L_2(\Omega)}} \approx \frac{\sqrt{h \cdot \Delta t \cdot \sum_{i,j} [\mathbf{y}_i(t_j) - \mathbf{y}^{(\ell,m)}_i(t_j)]^2}}{\sqrt{h \cdot \Delta t \cdot \sum_{i,j} \mathbf{y}_i^2(t_j)}},
\end{align}
where $h$ and $\Delta t$ are the step size in space and time, respectively. In the same way, we denote by $e_\ell$ the relative error that compares the full-order solution with an approximation obtained by the POD-reduced system \eqref{redSys} with the POD-dimension equal to $\ell$. In Figure \ref{relErr}, we present how the relative error behaves when either the DEIM-dimension is fixed (left) or the POD-dimension is fixed (right). From the numerical results that have been obtained for a viscosity parameter $\nu = 0.01$ and a full-order dimension of $N = 80$, we conclude that the error of the POD and the POD-DEIM reduced model are of the same size until the POD-dimension exceeds the DEIM-dimension which has been fixed in the left plot to $m = 15$. When $\ell$ is larger than $m$ in the left plot, we observe that only the POD-reduced model is still improved while the error of the POD-DEIM reduced model seems to be dominated by the error caused by the DEIM approximation of size $m = 15$. %Figure \ref{relErr} shows the behavior of the relative error as the POD dimension $\ell$ increases. We see from the plot that from $\ell = 11$ on, the relative error is of the order $\mathcal{O}(10^{-4})$. Since the full-order system consists of $N=50$ grid points in space, we conclude that in this specify setup the reduced order model of approximately one fifth of the size of the original system already leads to results of a very good accuracy.
\begin{figure}[H]
  \centering
  \subfloat{\includegraphics[width=0.49\textwidth]{plots/rel_Err_newleft}}\hfill
  \subfloat{\includegraphics[width=0.49\textwidth]{plots/rel_Err_newright}}\hfill
  \caption{Relative error of the POD and POD-DEIM approximation for a fixed number of DEIM basis $m = 15$ (left) and a fixed number of POD basis $\ell = 15$ (right).}\label{relErr}
\end{figure}
On the right of Figure \ref{relErr}, we observe the same behavior: An increase of the DEIM-dimension does not lead to a better approximation once the fixed POD-basis is overgrown. In Figure \ref{relErrml}, we observe this behavior for all combinations of $(\ell,m)$ in a range of \texttt{3:2:39}. Therefore, we conclude that for an optimal dimension reduction, we need to choose $\ell$ and $m$ in such a way that they are almost of the same size.
\begin{figure}[H]
  \centering
\includegraphics[width=0.7\textwidth]{plots/err_lm}
  \caption{Relative error of the POD-DEIM reduced model in dependence of the reduced dimensions $\ell$ and $m$.}\label{relErrml}
\end{figure}

\subsection{The usage of POD-DEIM for parameter studies}
In \cite{CS10}, the authors propose to use POD-DEIM also for parameter studies of a given dynamical system. In the case of Burgers' equation, this mean that we simulate the full-order system twice for two different viscosity parameters $\nu_{left} = 0.001$ and $\nu_{right} = 0.1$, using $N=400$ grid points in space which is necessary when dealing with very small values for $\nu$. The numerical solutions of the full-order system for both viscosity parameters have already been presented in Figure \ref{FullNumSol}. It can be seen that the size of the viscosity parameter plays a crucial role for the propagation of the initial impulse in time: For large $\nu$, the solution becomes much more diffusive and, therefore, the initial condition vanishes very fast. For small $\nu$, the initial condition is propagated in time almost without losing height.
%\begin{figure}[H]
%\includegraphics[width=0.99\textwidth]{plots/param_study_both}
%\caption{Full-order system for $\nu=0.001$ (left) and $\nu = 0.1$ (right).}\label{param_study_both}
%\end{figure}
The benefit of POD-DEIM for parameter studies is that a reduced model can be obtained for any parameter $\nu$ that lies within the interval $[\nu_{left},\nu_{right}]$ only from data of the full-order model corresponding to $\nu_{left}$ and $\nu_{right}$. Suppose we are interested in an approximation of the numerical solution for $100$ different viscosity parameter in $[\nu_{left},\nu_{right}]$, we only need to solve the full-order system twice and are able to obtain a reduced model from these data.

The reduction is based on a singular value decomposition of the combined matrices
\begin{align*}
Y = [Y_{\ell} | Y_{r}], \quad F = [F_{\ell} | F_{r}],
\end{align*}
where $Y_{\ell}$ and $F_{\ell}$ are snapshot matrices of the solution and the nonlinearity corresponding to $\nu_{left}$ and $Y_{r}$ and $F_r$ correspond to $\nu_{right}$, respectively. The POD-basis is then derived in the same way as described in Section \ref{BurgersPODDEIM} based on the snapshot matrix $Y$ that contains information of the dynamical behavior of Burgers' equation with viscosity parameter $\nu_{left}$ and $\nu_{right}$. Similarly, the DEIM projection matrix $\mathcal{P}$ is obtained from the matrix $F$.

In Figure \ref{param_study_inner}, the POD-DEIM approximation for the viscosity parameter $\nu = 0.01$ using the dimensions $\ell = m = 45$ is presented as well as the full-order model of dimension $N = 400$ for the same viscosity parameter. The plot shows the good quality of the reduced system. At this point, we want to stress that in order to obtain the POD-DEIM model in Figure \ref{param_study_inner}, the simulation of the full-order system with parameter $\nu = 0.01$  was not taken into account. Therefore, the derivation of a reduced model for any parameter $\nu \in [\nu_{left}, \nu_{right}]$ only requires the computational work of the numerical solution of the reduced system once the matrices $Y$ and $F$ are obtained.
\begin{figure}[H]
\includegraphics[width=0.99\textwidth]{plots/param_study_inner}
\caption{Full-order system and POD-DEIM reduced model for $\nu=0.01$.}\label{param_study_inner}
\end{figure}
\subsection{Comparison of the computational speedup for POD and POD-DEIM}
\label{sect2_numTests}
In the previous chapters, the main focus lies on the size of the reduced system and the quality of the approximation compared to the full-order system. In this section, we are interested in the speedup of the computation when POD and POD-DEIM is applied, respectively. Therefore, we consider the numerical solution of Burgers' equation as described in Appendix \ref{FEMDiscr} for different viscosity parameters $\nu$. From the numerical analysis of the full model, it is well-known that for different sizes of $\nu$ a different size of the spatial discretization is required in order to obtain a stable numerical solution. In \ref{Tab2}, we have chosen a minimal $N$ such that the full-order solution is stable.

The following quantities have been measured during the numerical simulation:
\begin{itemize}
  \item $t_{setup}$ - The time which is required to pre-compute the matrices \eqref{Bl}, \eqref{Cl} in the case of pure POD and \eqref{Bred}-\eqref{Fred} in the case of POD-DEIM. Note that this only has to be performed once.
  \item $t_{PDEsol}$  - The time for the numerical solution of Burgers' equation. This has been done via the implicit Euler method and a Newton iteration for the time integration and a finite elements approach for the spatial discretization.
  \item $e_\ell[10e^{-4}]$  - The relative error in $L_2([0,1] \times [0,1])$ as defined in \eqref{relErr_def}.
  \item $S_P^{(1)}$  - The overall speedup which is defined as the ratio of the computational time for the full-order model and the respective reduced model.
  \item $S_P^{(2)}$ - The speedup when only the numerical solution of Burgers' equation is taken into account. Here, we neglect the time that is required for the pre-computation in $t_{setup}$.
\end{itemize}

In order to obtain comparable results, we decided to choose the reduced dimensions $\ell$ and $m$ such that the relative error $e_{\ell,m}$ of the respective reduced model has in each case the same order of magnitude, $e_{\ell,m} \in \mathcal{O}(10^{-4})$. We, therefore, indicate in the first row of \mbox{Table \ref{Tab2}} the important dimension from the respective model: In case of the full-order model we present the number $N$ of ansatz function of the FEM discretization. For the pure POD reduced model we give the size of the POD-basis, $\ell$, and for the the POD-DEIM reduced model we present the dimensions as a tuple $(\ell,m)$.
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\cline{1-10}
 & \multicolumn{3}{ c| }{$\nu = 0.01$} & \multicolumn{3}{ c| }{$\nu = 0.001$}& \multicolumn{3}{ c| }{$\nu = 0.0001$}\\ \cline{2-10}
 & Full & POD & DEIM & Full & POD & DEIM & Full & POD & DEIM \\ \cline{1-10}
$N$/$\ell$/$m$ & $80$ &$ 11 $&$(11,13)$ & $200$&$35 $& $(35,40)$  & $800$&$40 $& $(40,55)$ \\ \cline{1-10}
$t_{setup}[s]$        & -      &0.003      &0.011       & -    &0.009&0.021 & -& 0.0729&0.182\\ \cline{1-10}
$t_{PDEsol}[s]$   &  0.068     &0.047      &0.040       & 0.232&0.12&0.055 & 4.416&0.276&0.085\\ \cline{1-10}
$e_{\ell,m}[10^{-4}]$   & -      &6.13       &6.46        & -    &6.12&6.85 & -&7.43& 7.66\\ \cline{1-10}
$S_P^{(1)}$           & -      &1.35       &1.34        & -    &1.74&3.03 & -&12.54&16.52\\ \cline{1-10}
$S_P^{(2)}$           & -      &1.44       &1.71        & -    &1.88&4.21 & -&15.85&51.85\\ \cline{1-10}
\end{tabular}
\caption{Comparison between POD and POD-DEIM for different values of $\nu$.} \label{Tab2}
\end{table}
We first note that for all different sizes of the viscosity parameter $\nu \in \{0.01, 0.001, 0.0001\}$ it is in general possible to derive a POD and POD-DEIM reduced model of a tremendously smaller dimension and a high accuracy of $e_{\ell,m} \in \mathcal{O}(10^{-4})$. The numerical tests of Table \ref{Tab2} are presented in order to prove two theoretical considerations: Firstly, we see that the time which is required for the numerical solution of the POD-DEIM reduced model does not increase when the size of the original problem, $N$, increases. On the other hand, we see that the solution of the pure POD model still depends of the original problem size and it rises when $N$ is increased. This behavior is stressed in Figure \ref{depN} where the increase of $N$ only affects the computation time of the POD model and, therefore, leads to a large speedup of the POD-DEIM reduced model. We see that for $N=3,000$, the POD-DEIM model is almost five times faster than the pure POD model. Secondly, we can conclude that in order to apply DEIM, it is more costly to apply the required pre-computations of the matrices \eqref{Bred}-\eqref{Fred}. This is mostly because two singular value decompositions are required in order to obtain the projection basis and the input basis for the DEIM algorithm \ref{alg:DEIM}. This is also the reason why the overall speedup $S_P^{(1)}$ of POD and POD-DEIM for the presented test configurations is comparable even though DEIM leads to a higher speedup. The most important observation of Table \ref{Tab2} is, however, that we are able to show a speedup of the POD-DEIM method of more than $50$ compared to the full model while POD only leads to a speedup of $\sim \! 16$. This speedup computation only takes into account the time that is required to actually solve the reduced system numerically. Therefore, we conclude that DEIM leads to a tremendous reduction of computational cost if we are able to pre-compute the matrices \eqref{Bred}-\eqref{Fred} only once and then solve the reduced system many times. This conclusion gives rise to the application of POD-DEIM within an optimization iteration as presented in the following chapter.
\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.49\textwidth]{plots/PODDEIM_Sp2}}\hfill
\subfloat{\includegraphics[width=0.49\textwidth]{plots/PODDEIM_Tsol}}\hfill
\caption{Dependence of the reduced model on the full-order dimension $N$.}\label{depN}
\end{figure}

%In the previous chapters, the main focus lies on the size of the reduced system and the quality of the approximation compared to the full-order system. In this section, we are interested in the speed-up of the computation when POD and POD-DEIM is applied. Therefore, we consider Burger's equation with viscosity parameter $\nu = 0.003$ on a spatial domain $[0,1]$ and end time $T = 4$. This requires a spatial and time discretization of $N = N_t =400$ points in order to obtain a stable numerical simulation. The reduced model with dimensions $\ell=m=45$ leads to an approximation of high accuracy (see Table \ref{Tab1}).
%
%The following quantities have been measured during the numerical simulation:
%\begin{itemize}
%  \item $t_s$ - The time which is required to pre-compute the matrices \eqref{Bl}, \eqref{Cl} in the case of pure POD and \eqref{Cl}, \eqref{Bred}, \eqref{Fred} in the case of POD-DEIM.
%  \item $t_{Eul}$ - The time for the time integration via the implicit Euler method and Newton's iteration.
%  \item $\bar e$ - The relative error in $L_2([0,1] \times [0,4])$.
%  \item $S_P$ - The overall speed-up which is defined as the ratio of the computational time for the full-order model and the reduced model.
%\end{itemize}
%In Table \ref{Tab1}, the speed-up of pure POD and POD-DEIM are compared: The application of POD of dimension $\ell = 45$ leads to a speed-up of the computation of approximately $3.5$ times the time of the original-size solution. If in addition DEIM is applied with dimension $m = 45$, we notice that the speed-up compared to the full-order model is $11.6$ which is approximately a speed-up of another $3.5$ times compared to the pure POD method. Therefore, in this setting the computational benefit of POD and POD-DEIM is almost of the same size. Note that both dimensions have been chosen in such a way that the relative error is of the order $\mathcal{O}(10^{-4})$ in both cases and, thus, the accuracy of the approximations is comparable for both cases.
%\begin{table}[H]
%\centering
%\begin{tabular}{l|c|c|c}
%& Full Order Model & POD & POD-DEIM \\
%\hline
%$t_s$ & - &  0.0018s& 0.0021s\\
%$t_{Eul}$ & 18.0785s& 5.1342s & 1.5189s\\
%$\bar e$ & - & 1.2805e-04 & 2.5057e-04\\
%$S_P$ & - &  3.5199 & 11.6801 \\
%\end{tabular}
%\caption{Comparison between POD and POD-DEIM for $\nu = 0.003$.} \label{Tab1}
%\end{table}
